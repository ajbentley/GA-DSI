{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Feature Importance for Random Forest\n",
    "Week 6| Lesson 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Explain how feature importance is calculated for decision trees\n",
    "- Calculate feature importance manually\n",
    "- Extract feature importance with scikit-learn\n",
    "- Extend the calculation to ensemble models (RF, ET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Perform a classification with Decision Trees\n",
    "- Perform a classification with Random Forest\n",
    "- Perform a classification with Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 mins | [Opening](#opening) | Opening |\n",
    "| 20 min | [Introduction](#introduction) | Feature importance for non-parametric models |\n",
    "| 30 min | [Demo](#demo) | Demo: Feature importance in Decision Trees |\n",
    "| 25 min | [Guided-practice](#guided-practice) | Guided Practice: Feature importance in Ensemble models |\n",
    "| 5 min | [Conclusion](#conclusion) | Conclusion |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"opening\"></a>\n",
    "## Opening (5 mins)\n",
    "\n",
    "When we build a machine learning model, we may be interested in more than just predictive accuracy.\n",
    "\n",
    "Often we are seeking insights on the relevant predictor variables.\n",
    "\n",
    "E.g. you have 1000 features to predict user retention.\n",
    "Which features are relevant? Can you identify them? Can you build marketing strategies to address them?\n",
    "\n",
    "> **Check:** Discuss with a partner -- how did we assess feature importance for e.g. logistic regression? What potential pitfalls are there in this process? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"introduction\"></a>\n",
    "## Feature importance for non-parametric models (20 min)\n",
    "\n",
    "We previously discussed feature selection in the context of logistic regression.\n",
    "\n",
    "Logistic regression is a *parametric model*, which means that our hypothesis is described by an assumed structure of coefficients that we tune to improve the model's accuracy. Since LR is a linear model, each parameter is associated to a specific feature.\n",
    "\n",
    "If the features are normalized, we can [interpret the size of each coefficient](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm) as indicating the relative importance of that specific feature. (The precise interpreration depends on how you've set up your model.)\n",
    "\n",
    "> **Check:** Which sklearn feature selections have we discussed? Name them and I'll write these on the board.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Decision Trees\n",
    "Tree based models are non-parametric, thus we don't have coefficients to tune like we did in linear models.\n",
    "\n",
    "We can however still ask which of the features are more important.\n",
    "\n",
    "> **Check:** How does a tree decide which split to perform?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: The decision tree algorithm makes locally optimal choices to maximize the gain in purity after the choice with respect to before the choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** What are some ways to measure purity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Answer: For classification we discussed Gini impurity and information gain/entropy.\n",
    "> For regression trees we used Mean Squared Error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When training a tree, we can compute how much each feature decreases the weighted impurity by adding up all the impurity gains where such a feature is used to determine a split.\n",
    "\n",
    "So the importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Random Forest\n",
    "\n",
    "Recall:\n",
    "\n",
    "- Each tree built on bootstrapped sample of data, and predictions aggregated (\"bagging\")\n",
    "- Each split in teach tree done on random subset of features (\"feature bagging\")\n",
    "\n",
    "> **Check:** How would you extend the definition of feature importance from decision trees to random forests? Discuss with your table.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Make a bunch of decision trees with random splits and then random forest decides which tree works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"demo\"></a>\n",
    "### Demo: Feature importance in Decision Trees (30 min)\n",
    "\n",
    "Suppose you are working at a car company and you are tasked to identify which features drive the acceptability of a car. You have collected some data on several features including:\n",
    "\n",
    "    - PRICE                  overall price\n",
    "        - buying             buying price\n",
    "        - maint              price of the maintenance\n",
    "    - TECH                   technical characteristics\n",
    "        - COMFORT            comfort\n",
    "            - doors          number of doors\n",
    "            - persons        capacity in terms of persons to carry\n",
    "            - lug_boot       the size of the trunk\n",
    "        - safety             estimated safety of the car\n",
    "\n",
    "(This is the car dataset we used previously.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Feature values are:\n",
    "\n",
    "    buying       v-high, high, med, low\n",
    "    maint        v-high, high, med, low\n",
    "    doors        2, 3, 4, 5-more\n",
    "    persons      2, 4, more\n",
    "    lug_boot     small, med, big\n",
    "    safety       low, med, high\n",
    "\n",
    "Class Distribution (number of instances per class):\n",
    "\n",
    "    class      N          N[%]\n",
    "    -----------------------------\n",
    "    unacc     1210     (70.023 %) \n",
    "    acc        384     (22.222 %) \n",
    "    good        69     ( 3.993 %) \n",
    "    v-good      65     ( 3.762 %) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **Check:** Conceptually, what do we need to do to evaluate feature importance with a decision tree model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First of all let's load it and map it to binary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    " \n",
    "df = pd.read_csv('./assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This time we will encode the features using a one hot encoding scheme, i.e. we will consider them as categorical variables.\n",
    "\n",
    "Since Scikit-learn requires numerical values here, we will also need to map the labels to numbers. We can use the `LabelEncoder` we've encountered other times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1210\n",
       "0     384\n",
       "1      69\n",
       "3      65\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))\n",
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train a decision tree on the whole dataset (ignore overfitting for the moment). Let's also artificially constrain the tree to be small so that we can visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 3, min_samples_split = 2) # Keep it shallow\n",
    "\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You may be able to visualize the tree via graphviz, if you've run the installation gantlet succesfully. (I haven't.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'create_png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e6acc2f08adf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m                 special_characters=True)  \n\u001b[0;32m     13\u001b[0m \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'create_png'"
     ]
    }
   ],
   "source": [
    "from sklearn.externals.six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "from IPython.display import Image\n",
    "dot_data = StringIO()  \n",
    "export_graphviz(dt, out_file=dot_data,  \n",
    "                feature_names=X.columns,  \n",
    "                class_names=le.classes_,  \n",
    "                filled=True, rounded=True,\n",
    "                proportion=True,\n",
    "                special_characters=True)  \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())  \n",
    "graph.create_png()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image](./assets/images/output_17_0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "df[df['persons']=='2'].acceptability.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first choice involves `person_2`. If the car only takes 2 people (`person_2` == 1) then the condition (our target class) is `unacceptable`. This happens in 33% of the cases. Note that the leaf under the `False` branch is 100% pure, and therefore its Gini measure is 0.0.\n",
    "\n",
    "On the other hand, if the car can hold more than 2 people, we will need to consider other choices. For example if the car is unsafe, then it's also unacceptable. And so on.\n",
    "\n",
    "> **Check:** What could be an advantage of using a decision tree in a model at work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Possible answer: easier to communicate results and understand relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the target is a classification taking values 0, 1, ... K-2, K-1. If node $m$ represents a region $R_m$ with $N_m$ observations, the proportion of class $k$ observations in node $m$ can be written as:\n",
    "$$\n",
    "        C_k = \\frac{1}{N_m} \\sum_{y_m\\text{ in }R_m} I(y_m = k)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Gini Index, measuring impurity,  is then defined as:\n",
    "$$\n",
    "        \\text{Gini}= \\sum_{k=0}^{K-1} C_k (1 - C_k)\n",
    "$$\n",
    "\n",
    "[This is](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) the sum of the probability of some class being chosen multiplied by the probability of an incorrect choice: it shows us how often you'd be wrong if you just guessed randomly from the classes in that sample, [conditional on some class being true](http://stats.stackexchange.com/questions/175087/basic-gini-impurity-derivation).\n",
    "\n",
    "Thanks to summation rules, and the fact that the individual probabilities for all classes must sum to 1, we can rearrange this as:\n",
    "\n",
    "$$\n",
    "\\sum_{k=0}^{K-1} (C_k - C_k^2) = \\sum_{k=0}^{K-1} C_k - \\sum_{k=0}^{K-1} C_k^2 = 1 - \\sum_{k=0}^{K-1} C_k^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot the gini index for various proportions in a binary classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "C0 = np.linspace(0,1)\n",
    "C1 = 1.0 - C0\n",
    "\n",
    "# Because it's binary, you can't do worse than .50\n",
    "gini = 1 - (C0**2 + C1**2)\n",
    "\n",
    "plt.plot(C0, gini)\n",
    "plt.title('Gini index for a Binary Classification') \n",
    "plt.xlabel('Fraction of samples in class 0')\n",
    "plt.ylabel('Gini index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's verify the calculation of the Gini index in the root node of the tree above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "root_node_values = [0.22, 0.04, 0.7, 0.04]\n",
    "\n",
    "def gini(values):\n",
    "    tot = 0.0\n",
    "    for val in values:\n",
    "        tot += val ** 2\n",
    "    \n",
    "    return 1.0 - tot\n",
    "\n",
    "gini(root_node_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Check:** Check that the value we obtained is the same as the one appearing in our decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Great, now we are ready to look at feature importances in our tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.columns\n",
    "print dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(dt.feature_importances_,\n",
    "                                   index = X.columns,\n",
    "                                    columns=['importance'])\n",
    "feature_importances.sort_values(by='importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we artificially constricted the tree to be small, only 3 features are used to make splits. Let's verify the calculation of the importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gini_gain_persons_2  = 1.0 * 0.45728376  - 0.667 * 0.57923569 - 0.333 * 0.0\n",
    "gini_gain_safety_low   = 0.666 * 0.57923569 - 0.444 * 0.62880113 - 0.222 * 0.0\n",
    "gini_gain_buying_vhigh = 0.444 * 0.62880113 - 0.333 * 0.6285747 - 0.111 * 0.46875\n",
    "\n",
    "# gini_gain_persons_2  = 1.0 * 0.45728376  - 0.667 * 0.57923569 - 0.333 * 0.0\n",
    "# gini_gain_safety_low   = 0.666 * 0.57923569 - 0.444 * 0.62880113 - 0.222 * 0.0\n",
    "# gini_gain_buying_vhigh = 0.444 * 0.62880113 - 0.333 * 0.6285747 - 0.111 * 0.46875\n",
    "\n",
    "# Calc total decrease in node impurity, weighted by prob of reaching that node (i.e. % of samples there)\n",
    "\n",
    "norm = gini_gain_persons_2 + gini_gain_safety_low + gini_gain_buying_vhigh\n",
    "\n",
    "print \"persons_2:\", gini_gain_persons_2 / norm\n",
    "print \"safety_low:\", gini_gain_safety_low / norm\n",
    "print \"buying_vhigh:\", gini_gain_buying_vhigh / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"guided-practice\"></a>\n",
    "## Guided Practice: Feature importance in Ensemble models (25 min)\n",
    "\n",
    "Scikit-learn [implements feature importance](http://stackoverflow.com/questions/15810339/how-are-feature-importances-in-randomforestclassifier-determined) for random forest and extra trees methods.\n",
    "\n",
    "Let's train one of each of these and investigate the feature importance:\n",
    "\n",
    "- Random Forest\n",
    "- Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', n_jobs=-1)\n",
    "et = ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "rf.fit(X, y)\n",
    "et.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Random forest exposes the feature importance and it calculates it as the average feature importance of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare the 3 models (re-init Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X, y)\n",
    "\n",
    "importances = pd.DataFrame(zip(dt.feature_importances_,\n",
    "                               rf.feature_importances_,\n",
    "                               et.feature_importances_),\n",
    "                           index=X.columns,\n",
    "                           columns=['dt_importance',\n",
    "                                    'rf_importance',\n",
    "                                    'et_importance']).sort_values('rf_importance',\n",
    "                                                                   ascending=False)\n",
    "\n",
    "                           \n",
    "importances.plot(kind='bar')\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Check:** Discuss in your tables the plot above. What are the common things across all models? What are the differences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusion (5 min)\n",
    "\n",
    "In this class we learned about feature importance and how they are calculated for tree based models.\n",
    "We have also deepened our understanding of the Gini measure.\n",
    "\n",
    "**Check:** How can you use this in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Gini Importance](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp)\n",
    "- [DT Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [Feature Importance in Sklearn Blog](http://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/)\n",
    "- [Plot Feature Importances example](http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "- [Selecting good features](http://blog.datadive.net/selecting-good-features-part-iii-random-forests/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
