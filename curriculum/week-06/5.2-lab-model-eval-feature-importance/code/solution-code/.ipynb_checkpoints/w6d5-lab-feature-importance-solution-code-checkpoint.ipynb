{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison Lab\n",
    "\n",
    "In this lab we will compare the performance of all the models we have learned about so far, using the car evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare the data\n",
    "\n",
    "The [car evaluation dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/) is in the assets/datasets folder. By now you should be very familiar with this dataset.\n",
    "\n",
    "1. Load the data into a pandas dataframe\n",
    "- Encode the categorical features properly: define a map that preserves the scale (assigning smaller numbers to words indicating smaller quantities)\n",
    "- Separate features from target into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df.buying.unique()\n",
    "print df.maint.unique()\n",
    "print df.lug_boot.unique()\n",
    "print df.safety.unique()\n",
    "print df.acceptability.unique()\n",
    "print df.persons.unique()\n",
    "print df.doors.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map1 = {'low':1,\n",
    "        'med':2,\n",
    "        'high':3,\n",
    "        'vhigh':4}\n",
    "map2 = {'small':1,\n",
    "        'med':2,\n",
    "        'big':3}\n",
    "map3 = {'unacc':1,\n",
    "        'acc':2,\n",
    "        'good':3,\n",
    "        'vgood':4}\n",
    "map4 = {'2': 2,\n",
    "        '4': 4,\n",
    "        'more': 5}\n",
    "map5 = {'2': 2,\n",
    "        '3': 3,\n",
    "        '4': 4,\n",
    "        '5more': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = [c for c in df.columns if c != 'acceptability']\n",
    "dfn = df.copy()\n",
    "\n",
    "dfn.buying= df.buying.map(map1)\n",
    "dfn.maint= df.maint.map(map1)\n",
    "dfn.lug_boot = df.lug_boot.map(map2)\n",
    "dfn.persons = df.persons.map(map4)\n",
    "dfn.doors = df.doors.map(map5)\n",
    "dfn.safety = df.safety.map(map1)\n",
    "dfn.acceptability = df.acceptability.map(map3)\n",
    "\n",
    "X = dfn[features]\n",
    "y = dfn['acceptability']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Useful preparation\n",
    "\n",
    "Since we will compare several models, let's write a couple of helper functions.\n",
    "\n",
    "1. Separate X and y between a train and test set, using 30% test set, random state = 42\n",
    "    - make sure that the data is shuffled and stratified\n",
    "2. Define a function called `evaluate_model`, that trains the model on the train set, tests it on the test, calculates:\n",
    "    - accuracy score\n",
    "    - confusion matrix\n",
    "    - classification report\n",
    "3. Initialize a global dictionary to store the various models for later retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "def evaluate_model(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    a = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cr = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print cm\n",
    "    print cr\n",
    "    \n",
    "    return a\n",
    "\n",
    "all_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.a KNN\n",
    "\n",
    "Let's start with `KNeighborsClassifier`.\n",
    "\n",
    "1. Initialize a KNN model\n",
    "- Evaluate it's performance with the function you previously defined\n",
    "- Find the optimal value of K using grid search\n",
    "    - Be careful on how you perform the cross validation in the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "a = evaluate_model(KNeighborsClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "params = {'n_neighbors': range(2,60)}\n",
    "\n",
    "gsknn = GridSearchCV(KNeighborsClassifier(),\n",
    "                     params, n_jobs=-1,\n",
    "                     cv=KFold(len(y), n_folds=3, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsknn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsknn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsknn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(gsknn.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models['knn'] = {'model': gsknn.best_estimator_,\n",
    "                     'score': a}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Bagging + KNN\n",
    "\n",
    "Now that we have found the optimal K, let's wrap `KNeighborsClassifier` in a BaggingClassifier and see if the score improves.\n",
    "\n",
    "1. Wrap the KNN model in a Bagging Classifier\n",
    "- Evaluate performance\n",
    "- Do a grid search only on the bagging classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "baggingknn = BaggingClassifier(KNeighborsClassifier())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(baggingknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bagging_params = {'n_estimators': [10, 20],\n",
    "                  'max_samples': [0.7, 1.0],\n",
    "                  'max_features': [0.7, 1.0],\n",
    "                  'bootstrap_features': [True, False]}\n",
    "\n",
    "\n",
    "gsbaggingknn = GridSearchCV(baggingknn,\n",
    "                            bagging_params, n_jobs=-1,\n",
    "                            cv=KFold(len(y), n_folds=3, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbaggingknn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbaggingknn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_models['gsbaggingknn'] = {'model': gsbaggingknn.best_estimator_,\n",
    "                              'score': evaluate_model(gsbaggingknn.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression\n",
    "\n",
    "Let's see if logistic regression performs better\n",
    "\n",
    "1. Initialize LR and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "all_models['lr'] = {'model': lr,\n",
    "                    'score': evaluate_model(lr)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
    "          'penalty': ['l1', 'l2']}\n",
    "\n",
    "gslr = GridSearchCV(lr,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gslr.fit(X, y)\n",
    "\n",
    "print gslr.best_params_\n",
    "print gslr.best_score_\n",
    "\n",
    "all_models['gslr'] = {'model': gslr.best_estimator_,\n",
    "                             'score': evaluate_model(gslr.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbagginglr = GridSearchCV(BaggingClassifier(gslr.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbagginglr.fit(X, y)\n",
    "\n",
    "print gsbagginglr.best_params_\n",
    "print gsbagginglr.best_score_\n",
    "\n",
    "all_models['gsbagginglr'] = {'model': gsbagginglr.best_estimator_,\n",
    "                             'score': evaluate_model(gsbagginglr.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Trees\n",
    "\n",
    "Let's see if Decision Trees perform better\n",
    "\n",
    "1. Initialize DT and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "all_models['dt'] = {'model': dt,\n",
    "                    'score': evaluate_model(dt)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'splitter': ['best', 'random'],\n",
    "          'max_depth': [None, 5, 10],\n",
    "          'min_samples_split': [2, 5],\n",
    "          'min_samples_leaf': [1, 2, 3]}\n",
    "\n",
    "gsdt = GridSearchCV(dt,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsdt.fit(X, y)\n",
    "print gsdt.best_params_\n",
    "print gsdt.best_score_\n",
    "\n",
    "all_models['gsdt'] = {'model': gsdt.best_estimator_,\n",
    "                      'score': evaluate_model(gsdt.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbaggingdt = GridSearchCV(BaggingClassifier(gsdt.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbaggingdt.fit(X, y)\n",
    "\n",
    "print gsbaggingdt.best_params_\n",
    "print gsbaggingdt.best_score_\n",
    "\n",
    "all_models['gsbaggingdt'] = {'model': gsbaggingdt.best_estimator_,\n",
    "                             'score': evaluate_model(gsbaggingdt.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Support Vector Machines\n",
    "\n",
    "Let's see if SVM perform better\n",
    "\n",
    "1. Initialize SVM and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n",
    "- See if Bagging improves the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "all_models['svm'] = {'model': svm,\n",
    "                     'score': evaluate_model(svm)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'C': [0.01, 0.1, 1.0, 10.0, 30.0, 100.0],\n",
    "          'gamma': ['auto', 0.1, 1.0, 10.0],\n",
    "          'kernel': ['linear', 'rbf']}\n",
    "\n",
    "\n",
    "gssvm = GridSearchCV(svm,\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gssvm.fit(X, y)\n",
    "print gssvm.best_params_\n",
    "print gssvm.best_score_\n",
    "\n",
    "all_models['gssvm'] = {'model': gssvm.best_estimator_,\n",
    "                      'score': evaluate_model(gssvm.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gsbaggingsvm = GridSearchCV(BaggingClassifier(gssvm.best_estimator_),\n",
    "                           bagging_params, n_jobs=-1,\n",
    "                           cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsbaggingsvm.fit(X, y)\n",
    "\n",
    "print gsbaggingsvm.best_params_\n",
    "print gsbaggingsvm.best_score_\n",
    "\n",
    "all_models['gsbaggingsvm'] = {'model': gsbaggingsvm.best_estimator_,\n",
    "                             'score': evaluate_model(gsbaggingsvm.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Random Forest & Extra Trees\n",
    "\n",
    "Let's see if Random Forest and Extra Trees perform better\n",
    "\n",
    "1. Initialize RF and ET and test on Train/Test set\n",
    "- Find optimal params with Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "all_models['rf'] = {'model': rf,\n",
    "                    'score': evaluate_model(rf)}\n",
    "\n",
    "\n",
    "\n",
    "et = ExtraTreesClassifier()\n",
    "all_models['et'] = {'model': et,\n",
    "                    'score': evaluate_model(et)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators':[3, 5, 10, 50],\n",
    "          'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': [None, 3, 5],\n",
    "          'min_samples_split': [2,5],\n",
    "          'class_weight':[None, 'balanced']}\n",
    "\n",
    "\n",
    "gsrf = GridSearchCV(RandomForestClassifier(n_jobs=-1),\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gsrf.fit(X, y)\n",
    "print gsrf.best_params_\n",
    "print gsrf.best_score_\n",
    "\n",
    "all_models['gsrf'] = {'model': gsrf.best_estimator_,\n",
    "                      'score': evaluate_model(gsrf.best_estimator_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gset = GridSearchCV(RandomForestClassifier(n_jobs=-1),\n",
    "                    params, n_jobs=-1,\n",
    "                    cv=KFold(len(y), n_folds=3, shuffle=True))\n",
    "\n",
    "gset.fit(X, y)\n",
    "print gset.best_params_\n",
    "print gset.best_score_\n",
    "\n",
    "all_models['gset'] = {'model': gset.best_estimator_,\n",
    "                      'score': evaluate_model(gset.best_estimator_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model comparison\n",
    "\n",
    "Let's compare the scores of the various models.\n",
    "\n",
    "1. Do a bar chart of the scores of the best models. Who's the winner on the train/test split?\n",
    "- Re-test all the models using a 3 fold stratified shuffled cross validation\n",
    "- Do a bar chart with errorbars of the cross validation average scores. is the winner the same?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = pd.DataFrame([(k, v['score']) for k, v in all_models.iteritems()],\n",
    "             columns=['model', 'score']).set_index('model').sort_values('score', ascending=False)\n",
    "\n",
    "\n",
    "scores.plot(kind='bar')\n",
    "plt.ylim(0.6, 1.1)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "\n",
    "def retest(model):\n",
    "    scores = cross_val_score(model, X, y,\n",
    "                             cv=StratifiedKFold(y, shuffle=True),\n",
    "                             n_jobs=-1)\n",
    "    m = scores.mean()\n",
    "    s = scores.std()\n",
    "    \n",
    "    return m, s\n",
    "\n",
    "for k, v in all_models.iteritems():\n",
    "    cvres = retest(v['model'])\n",
    "    print k, \n",
    "    all_models[k]['cvres'] = cvres\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvscores = pd.DataFrame([(k, v['cvres'][0], v['cvres'][1] ) for k, v in all_models.iteritems()],\n",
    "                        columns=['model', 'score', 'error']).set_index('model').sort_values('score', ascending=False)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(range(len(cvscores)), cvscores.score,\n",
    "                yerr=cvscores.error,\n",
    "                tick_label=cvscores.index)\n",
    "\n",
    "ax.set_ylabel('Scores')\n",
    "plt.xticks(rotation=70)\n",
    "plt.ylim(0.6, 1.1)\n",
    "\n",
    "# cvscores.to_csv('../../../5.2-lesson/assets/datasets/car_evaluation/model_comparison.csv')\n",
    "cvscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "We have encoded the data using a map that preserves the scale.\n",
    "Would our results have changed if we had encoded the categorical data using `pd.get_dummies` or `OneHotEncoder`  to encode them as binary variables instead?\n",
    "\n",
    "1. Repeat the analysis for this scenario. Is it better?\n",
    "- Experiment with other models or other parameters, can you beat your classmates best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "onehotpipe = make_pipeline(OneHotEncoder(),\n",
    "                           dt)\n",
    "\n",
    "\n",
    "all_models['onehotpipe'] = {'model': onehotpipe,\n",
    "                            'score': evaluate_model(onehotpipe)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
