{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Park Slope Parents Membership Project</h1></center>\n",
    "<center> <h2> Descriptive Analysis and Predictive Model for Membership Longevity </h2></center>\n",
    "<hr>\n",
    "<center> <h2> Summary and Technical Analysis</h2></center>\n",
    "<center> <h3> Winter 2016-2017 </n> A. Joshua Bentley, DSI-2 Capstone</h3></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "%matplotlib inline\n",
    "from datetime import datetime, date, timedelta\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from plotly.graph_objs import graph_objs\n",
    "from plotly.tools import FigureFactory as FF\n",
    "from sklearn import cluster, datasets, preprocessing, metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn import linear_model, metrics\n",
    "from sklearn.cluster import DBSCAN, Kmeans\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier,  RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, PolynomialFeatures, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plotly.graph_objs import graph_objs\n",
    "import plotly.graph_objs as go\n",
    "import plotly.plotly as py\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.style.use('seaborn-white')\n",
    "# plot.ly credentials withheld for security. Images of results supplied.\n",
    "wine_palette = sns.xkcd_palette(['dandelion', 'claret'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "Park Slope Parents, a networking and peer support group for parents living in and around the Park Slope Neighborhood of Brooklyn has seen a flattening of membership and was interested in better understanding when families joined and left the group. Specifically they had these questions:\n",
    "\n",
    "* When do people typically join PPP relative to their children’s birth dates?\n",
    "* When do people typically let membership lapse?\n",
    "* What is the average length of membership?\n",
    "* Is there seasonality associated with membership?\n",
    "* How have membership levels changed over the years?\n",
    "* How has the group’s reach changed over the years?\n",
    "* How can we continue to grow?\n",
    "\n",
    "## Methodology\n",
    "My goal is to address and expand upon these questions in five phases. The first two are covered in this paper while the remaining phases are behyond the client's initial scope and therefor will be addressed in the future.\n",
    "\n",
    "Phase I: Descriptive Analysis\n",
    "\n",
    "Phase II: Modelling Long Term versus Short Term membership\n",
    "\n",
    "Phase III: NLP\n",
    "\n",
    "Phase IV: Demographic shifts in Brooklyn for marketing efforts\n",
    "\n",
    "Phase V: Membership price analysis\n",
    "\n",
    "## Phase I: Descriptive Analysis\n",
    "After removing open-response fields (address and reasons for joining the group), which I felt would be more useful in future phases, I dummied out several groups that had string responses where int would be more useful in analysis, then deleted the original column of data. Where the answers were binary (e.g., Yes/No, male/female) I deleted one value of the pair.\n",
    "\n",
    "I chose to take this route with the binary response columns rather than converting the responses to 0 and 1 because it seemed clearer code, though perhaps less \"Pythonic.\"\n",
    "\n",
    "I added columns for the month and year a member joined, the member's first child was born, and the member's second child was born (if only one child I imputed the first child's month and year). I created a new column for the year the membership lapsed. While doing this I discovered several outliers. Some were the result of input error which reported children as having been born in the year 2 (002) ranther than 2002 (2002 or 02). \n",
    "\n",
    "There were also situations where grandparents would join the organization and when they were asked for their childen's birthdays would answer accuratly, meaning they would give their childen's birthdays and not their grandchildren's birthdays. In order to account for this I limited the study to members who had children whose birthdays were between 1995 - 2018. \n",
    "\n",
    "My first round of EDA was histograms taken directly from the data on month joined, number of children, birth patterns by month, how the group was discovered, and membership type (options were available for 1, 2, 5, and lifetime memberships). Most visualizations are plot.ly initiated in python and then cleaned in plot.ly workshop. Stacked 100% cylander charts (or, as the client calls them, Smartees charts) were made in Excel.\n",
    "\n",
    "#### Descriptive Analysis Observations and Visualizations\n",
    "Code here was fairly straighforward and is provided following the write-up. All plot.ly code is commented out as it requires sign-in to run and my credentials have not been provided.\n",
    "\n",
    "** 1. When do families join? **\n",
    "<br>\n",
    "<br>\n",
    "Typically membership is fairly even through the year with November and December lagging as the lowest months for new members joining. Perhaps because of these softer numbers January tends to be stronger than most for new families.\n",
    "\n",
    "New families also seem to join in the summer months. According to PPP this is largely for nanny recommendations.\n",
    "\n",
    "This year the summer swell was unusually strong as nearly half of the year's new families joined between May and July.\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/join_month_year.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "** 2. How many children do PPP families have? **\n",
    "<br>\n",
    "<br>\n",
    "Most families have had only one child, but the number of two children families has been very strong as well. I had been toying with the idea of just using 1-child families since that would be easier for some comparisons, but that is clearly not an option here. The good news is that there are very few 3+ families so the fact that we don't have birth dates past #2 is less of an issue.\n",
    "\n",
    "Please note that the original distribution for this feature included a \"more than four\" category. There were few enough families of that category that I merged them with the 4 (to make 4+)\n",
    "\n",
    "Additionally the 0.5 column is for families who had a child on the way, through pregnancy or adoption, but were not yet parents.\n",
    "\n",
    "Mean number of children for Parkside Parenting Partnership families has been 1.37\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_kidcount.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Looking at how the composition of the membership has changed over time in terms of number of children, the Parkside Parenting Partnership is increasingly comprised of families with one child.\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/kid_count_yr.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "** 3. When are members' children born? **\n",
    "<br>\n",
    "<br>\n",
    "Birth patterns are very similar for first and second children, never more than 50 children between number born in each month.\n",
    "\n",
    "Please note, this does not mean that individual families have children in the same month, just that overall patterns are similar.\n",
    "\n",
    "This section is largely here for the benefit of the client and has a considerable bias as when there was no second child the first child's birthdate is used. Regardless, there are enough second children that a discrepency should be apparent if it somehow followed a different pattern.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_birthmonth.png?raw=true)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "** 4. How did members find out about the Parkside Parenting Partnership? **\n",
    "<br>\n",
    "<br>\n",
    "Far and away the top means of finding PPP was through a friend/neighbor who is a member of the group.\n",
    "<br>\n",
    "<br>\n",
    "<center>*Data Dictionary*</center>\n",
    "\n",
    "|Label\t |  Definition|\n",
    "|--------|------------|\n",
    "|0  |\t A PPP member I don\\'t know told me about it|\n",
    "|1\t |  A PPPP member who is a friend/neighbor|\n",
    "|2\t |  Found it through Yahoo|\n",
    "|3\t |  Found it through a Google search|\n",
    "|4\t |  Heard about it on another online parenting group (Urban Baby, etc.)|\n",
    "|5\t |  Heard about it through a magazine, newspaper, blog|\n",
    "|6\t |  I don't remember / Other|\n",
    "|7\t |  NA|\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_discovered.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Finding PPP through a Google search was the #3 means of finding PPP and has been fairly consistent in most years, following a similar pattern to joining the group in most years.\n",
    "\n",
    "In 2016 the likelihood of a family having found PPP this way has been much less until May, dropping off again after July, mirroring the pattern seen for new families joining this year.  \n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/google_found_year.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "Interestingly, the trend of searches for the group as reported by Google are very different than the responses from families joining PPP. Searches in general have been on the decline, though this may merely suggest that people say they found PPP through Google when they just meant that they found it online somehow.\n",
    "\n",
    "These data collected from Google Trends\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_google_search.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "** 5. How popular is each type of membership? **\n",
    "<br>\n",
    "<br>\n",
    "In terms of percent of all families, 1-year memberships have been on a slow decline over the years with 2-year memberships on the rise.\n",
    "\n",
    "When she found out about this the client let me know that she is interested in looking into whether the price point for 2 year memberships is costing the organization more money than it would lose by having only 1 year memberships available. This is something that I will address in Phase V of the study.\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/mem_type_yr.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "** 6. How long do people keep their memberships?**\n",
    "<br>\n",
    "<br>\n",
    "Most members have only stayed for a year.\n",
    "\n",
    "Still, enough have been members through several years that the average is 2.83 years\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_memduration.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "** 7. When do members join the Parkside Parenting Partnership, relative to the arrival of their child?**\n",
    "<br>\n",
    "<br>\n",
    "Looking just at members who show a join date at most 12 years before the child's arrival or less than 2 years prior, we can see that the average (median) member joins about 5 months after their first child is born. This average includes people who join before their children are due.\n",
    "\n",
    "About 2/3 of members join after their children arrived.\n",
    "\n",
    "Among these members the average join date is 22 months old. That means that half of members who join after their children have arrived do so after age 22 months.\n",
    "\n",
    "The 1st quartile in this measurement, the 25% earliest joins post arrival, was at 4.5 months old. That is a large drop-off and presents a potential target for membership.\n",
    "\n",
    "Among members who join before their children arrive, they do so 3.6 months before the arrival. The 1st quartile here is 5.3 months so there isn't as much opportunity to reach out here.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_joinvbirth.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "Over time we can see that it has become more and more popular for people to join PPP while they are still waiting for their child to be born or adopted, representing over half of the members joining in 2016.\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/jvb_yrly.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "** 8. When do memberships lapse, relative to the arrival of members' latest children? **\n",
    "<br>\n",
    "<br>\n",
    "Looking only at expired memberships, they tend to last about 22 months after their second child was born (if applicable--if not then 1st child birth date was used).\n",
    "\n",
    "This is a _very_ interesting number as it is also the median age at which new members join (join date versus 1st child's birth). This suggests there may be a \"ships in the night\" pattern and makes it doubly important that Parenting Network concentrates on either increasing or emphasizing value for parents of children under 2 years old in order to both retain membership past this age and to bring parents in a little sooner.\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/psp_expvbirth.png?raw=true)\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "It appears that families are staying a little longer relative to their children's ages with a slight uptick in expirations after age 5 and slightly fewer in terms of 1-2 year olds.\n",
    "<br>\n",
    "<br>\n",
    "![](https://ajbentley.github.io/assets/images/psp/evb_yrly.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Technical Challenges**\n",
    "I hadn't really had to work with datetime information before. Previously when I had to get a piece of information it would just be a year, which I would get by referencing a slice of a string. Here I needed to use the entire date.\n",
    "\n",
    "What was a major problem was that it was difficult to make calculations off of the timedeltas that were the result of addition / subtraction on datetimes, but division / multiplication of them resulted in integers, though it was not clear what those integers represented. I found that if I divided it by 1440 I would get the number of days it represented.\n",
    "\n",
    "One thing that I needed to do in order to understand the organization's composition over time, as well as to answer the question of how membership levels changed year to year, was create breakouts for membership in each year. The data I had gave a start date and an end date which generally spanned a number of years. I tried a number of different ways to do this and was generally successful except for 2016, which I could not isolate. \n",
    "\n",
    "My first attempt only identified the year the member joined, which wasn't useful as I had that data already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### first attempt at isolating membership years\n",
    "### stopped indicating membership after it found a first year\n",
    "\n",
    "for year in year_list:\n",
    "\n",
    "    mem_year_list = []\n",
    "    \n",
    "    for x in dfy.join_year:\n",
    "        while x == year:\n",
    "            if x >= year | x < (year+1):\n",
    "                mem_year_list.append(1)\n",
    "            else:\n",
    "                mem_year_list.append(0)\n",
    "\n",
    "    myl = pd.Series(mem_year_list)\n",
    "    dfy[year] = myl\n",
    "    er = str(year)\n",
    "    er = ('mem_'+ er[2:])\n",
    "    dfy.rename (columns={year:er}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My next attempt at it showed a steep decline in 2016. When I asked the client about it she said that this wasn't true, according to data she had from a report from her site host. I went back and saw that the method I'd used to do this excluded members whose expiration was further out than 2017. When I tried to fix it it showed membership doubling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annual_mem_cols = ['mem_no','joined', 'exp_date', 'exp_year']\n",
    "dfy = pd.DataFrame(dfn[annual_mem_cols])\n",
    "\n",
    "dfy['joined'] = pd.to_datetime(dfy['joined'])\n",
    "dfy['exp_date'] = pd.to_datetime(dfy['exp_date'])\n",
    "\n",
    "year_list = [2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012,\\\n",
    "             2013, 2014, 2015, 2016]\n",
    "\n",
    "for year in year_list:\n",
    "    \n",
    "    dfy['mem_' + str(year)] = dfy.apply(lambda x: x['joined'].year <= year and x['exp_date'].\\\n",
    "                                        year>= year, axis=1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided that since that method had worked for all years except for 2016 I took 2016 out of that code and made a new code to bring in 2016 data. These figures still didn't jibe with what the client had. After sorting through the code in Excel and picking things out one by one I found that my numbers were correct based on the data provided. The client and I agreed to move forward with these data and to discuss the disconnect with her provider at a later date. \n",
    "\n",
    "Below is the code used to isolate members in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memlist_2016 = []\n",
    "\n",
    "for n in dfn.exp_year:\n",
    "    if n >= 2016:\n",
    "        memlist_2016.append(1)\n",
    "    else:\n",
    "        memlist_2016.append(0)\n",
    "\n",
    "        \n",
    "m16 = pd.Series(memlist_2016)\n",
    "        \n",
    "dfy = pd.concat([dfy, m16], axis = 1)\n",
    "dfy.rename (columns={0:'mem_2016'}, inplace=True)\n",
    "dfy.head()\n",
    "\n",
    "# plot the year to year changes\n",
    "mem_by_year_list = ['mem_2010','mem_2011','mem_2012','mem_2013','mem_2014','mem_2015','mem_2016']\n",
    "\n",
    "dfmc = pd.DataFrame(dfn[mem_by_year_list])\n",
    "dfmc.columns = ['2010', '2011', '2012', '2013', '2014', '2015', '2016']\n",
    "memcount =[]\n",
    "memcount = pd.Series(dfmc.sum(axis=0))\n",
    "print memcount\n",
    "\n",
    "memcount.plot(kind='line', figsize=(8,4), title=\"Annual PPP Membership\", fontsize=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nontechnical Challenges**\n",
    "During EDA I noticed trend changes that needed to be addressed. I contacted the client and was informed that in 2007 they got a website and in 2009 began charging for membership. After exploring year to year data I concluded that these changes were disruptive / significant and chose to cut data trends prior to 2010.\n",
    "\n",
    "After publishing my Phase I results on my website my client became upset that information about her group were being released to the public. I had not discussed this with her previously and had not thought it would be an issue. As a compromise I changed the name of the group in published work to \"Parkside Parenthood Partnership.\" My hope is that in the future she will allow me to change the name back so that people looking at my portfolio will know that it was real work and not an exercise on dummy data.\n",
    "\n",
    "As part of my data collection for Phase III I needed to be made an admin for a Slack Group for new parents. The members of the group were upset by someone being \"in their midst\" when talking about private issues. This was a good experience for this level in my work to learn that if I have to do something similar in the future the members should be advised of the reason for my presence and that I am collecting metadata, not reading individual conversations. It was a very active example of the [Hawthorne effect](https://en.wikipedia.org/wiki/Hawthorne_effect) (an observer effect similar to Heisenberg's uncertainty principle). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase I Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "# dfn = pd.read_csv(\"../../projects/psp/raw_data/PSP_data_4capstone.csv\")\n",
    "# dfn = pd.read_csv(\"../../projects/psp/refined_data/psp_numerical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert datetime columns\n",
    "\n",
    "dfn.joined = pd.to_datetime(dfn.joined, format = '%Y/%m/%d')\n",
    "dfn.exp_date = pd.to_datetime(dfn.exp_date, format = '%Y/%m/%d')\n",
    "dfn.last_renewal_date = pd.to_datetime(dfn.last_renewal_date, format = '%Y/%m/%d')\n",
    "dfn.kid1_bday = pd.to_datetime(dfn.kid1_bday, format = '%m/%d/%y')\n",
    "dfn.kid2_bday = pd.to_datetime(dfn.kid2_bday, format = '%m/%d/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting member number as index\n",
    "dfn.set_index('mem_no')\n",
    "\n",
    "# getting rid of a few columns\n",
    "dfn.drop([\"address\",'join_reason'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a few dummies\n",
    "\n",
    "status_dummy = pd.get_dummies(dfn['status'], prefix = 'status')\n",
    "memtype_dummy = pd.get_dummies(dfn['mem_type'], prefix = 'mem_type')\n",
    "email_dummy = pd.get_dummies(dfn['club_email'], prefix = 'club_email')\n",
    "dup_dummy = pd.get_dummies(dfn['dup'], prefix = 'dup')\n",
    "parent_dummy = pd.get_dummies(dfn['parent_status'], prefix = 'parent_status')\n",
    "\n",
    "advice_dummy = pd.get_dummies(dfn['advice_grp'], prefix = 'advice_grp')\n",
    "classifieds_dummy = pd.get_dummies(dfn['classifieds'], prefix = 'classifieds')\n",
    "class_sp_dummy = pd.get_dummies(dfn['classifieds_spouse'], prefix = 'classifieds_spouse')\n",
    "tony_dummy = pd.get_dummies(dfn['tony_kids'], prefix = 'tony_dids')\n",
    "disc_dummy = pd.get_dummies(dfn['discovered'], prefix = 'discovered')\n",
    "\n",
    "\n",
    "dfn = dfn.join(status_dummy)\n",
    "dfn = dfn.join(memtype_dummy)\n",
    "dfn = dfn.join(email_dummy)\n",
    "dfn = dfn.join(dup_dummy)\n",
    "dfn = dfn.join(parent_dummy)\n",
    "dfn = dfn.join(advice_dummy)\n",
    "dfn = dfn.join(classifieds_dummy)\n",
    "dfn = dfn.join(class_sp_dummy)\n",
    "dfn = dfn.join(tony_dummy)\n",
    "dfn = dfn.join(disc_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns for joined month and year\n",
    "\n",
    "dfn['join_year'] = dfn['joined'].dt.year\n",
    "dfn['join_month'] = dfn['joined'].dt.month\n",
    "\n",
    "# columns for exp_date year\n",
    "\n",
    "dfn['exp_year'] = dfn['exp_date'].dt.year\n",
    "\n",
    "# # columns for 1st kid's birth month and year\n",
    "dfn['k1bday_year'] = dfn['kid1_bday'].dt.year\n",
    "dfn['k1bday_month'] = dfn['kid1_bday'].dt.month\n",
    "\n",
    "# # columns for 2nd kid's birth month and year\n",
    "dfn['k2bday_year'] = dfn['kid2_bday'].dt.year\n",
    "dfn['k2bday_month'] = dfn['kid2_bday'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check dates for out of consideration range. basically for grandparents who are using their children's birth dates,\n",
    "# not their grandchildren's. Org started in 2002 so will assume anything prior to 1990 will be out of range\n",
    "\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k1bday_year'] >= 1995])\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k2bday_year'] >= 1995])\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k1bday_year'] < 2018])\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k2bday_year'] < 2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enumerate some columns that had been dummied so that they come up on histograms\n",
    "\n",
    "dfn['mem_type'].replace('1 year membership ($40)', 0, inplace=True)\n",
    "dfn['mem_type'].replace('2 Year Membership ($75)', 1, inplace=True)\n",
    "dfn['mem_type'].replace('3 year membership ($110)', 2, inplace=True)\n",
    "dfn['mem_type'].replace('5 year membership ($175)', 3, inplace=True)\n",
    "dfn['mem_type'].replace('Complimentary', 4, inplace=True)\n",
    "dfn['mem_type'].replace('Lifetime Member', 5, inplace=True)\n",
    "dfn['mem_type'].replace('Trial Membership', 6, inplace=True)\n",
    "\n",
    "dfn['parent_status'].replace('No', 0, inplace=True)\n",
    "dfn['parent_status'].replace('No, but we are pregnant/adopting', 1, inplace=True)\n",
    "dfn['parent_status'].replace('Yes', 2, inplace=True)\n",
    "\n",
    "dfn['discovered'].replace('A PSP member I don\\'t know told me about it', 0, inplace=True)\n",
    "dfn['discovered'].replace('A PSP member who is a friend/neighbor', 1, inplace=True)\n",
    "dfn['discovered'].replace('Found it through Yahoo', 2, inplace=True)\n",
    "dfn['discovered'].replace('Found it through a Google search', 3, inplace=True)\n",
    "dfn['discovered'].replace('Heard about it on another online parenting group (Urban Baby, etc.)', 4, inplace=True)\n",
    "dfn['discovered'].replace('Heard about it through a magazine, newspaper, blog', 5, inplace=True)\n",
    "dfn['discovered'].replace('I don\\'t remember', 6, inplace=True)\n",
    "dfn['discovered'].replace('NA', 5, inplace=True)\n",
    "dfn['discovered'].replace('Other', 6, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. When do members join? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# historam of month joined\n",
    "# data = [go.Histogram(x=dfn.join_month)]\n",
    "# py.iplot(data, thickness=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create pivot table to show count of joins per month per year.\n",
    "dfn_p = pd.pivot_table(dfn, values='mem_no', index='join_year', columns='join_month',\\\n",
    "               aggfunc=len, fill_value=0)\n",
    "\n",
    "# exported to Excel to create stacked bar graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2. How many children to PPP families have?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of number of children\n",
    "# data = [go.Histogram(x=dfn.kid_count)]\n",
    "# py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph for number of children broken out by annual membership\n",
    "\n",
    "num_child_cols = ['mem_no','kid_count', 'mem_2010','mem_2011','mem_2012','mem_2013',\\\n",
    "                  'mem_2014','mem_2015','mem_2016']\n",
    "\n",
    "dfn_num_child = pd.DataFrame(dfn[num_child_cols])\n",
    "\n",
    "# count of members with child due by year\n",
    "df_due = pd.DataFrame(dfn_num_child.loc[dfn_num_child['kid_count'] == 0.5])\n",
    "due_count = df_due.sum(axis=0)\n",
    "due = pd.Series(due_count)\n",
    "print due_count\n",
    "\n",
    "# count of members with 1 child per year\n",
    "df_one = pd.DataFrame(dfn_num_child.loc[dfn_num_child['kid_count'] == 1.0])\n",
    "one_count = df_one.sum(axis=0)\n",
    "one = pd.Series(one_count)\n",
    "print one_count\n",
    "\n",
    "# count of members with 2 children by year\n",
    "df_two = pd.DataFrame(dfn_num_child.loc[dfn_num_child['kid_count'] == 2.0])\n",
    "two_count = df_two.sum(axis=0)\n",
    "two = pd.Series(two_count)\n",
    "print two_count\n",
    "\n",
    "# count of members with 3 children by year\n",
    "df_three = pd.DataFrame(dfn_num_child.loc[dfn_num_child['kid_count'] == 3.0])\n",
    "three_count = df_three.sum(axis=0)\n",
    "three = pd.Series(three_count)\n",
    "print three_count\n",
    "\n",
    "# count of members with 4 children by year\n",
    "df_four = pd.DataFrame(dfn_num_child.loc[dfn_num_child['kid_count'] == 4.0])\n",
    "kfour_count = df_four.sum(axis=0)\n",
    "four_or_more = pd.Series(kfour_count)\n",
    "print kfour_count\n",
    "\n",
    "# concat kid count dfs\n",
    "df_kids = pd.concat([due, one, two, three, four_or_more], axis=1)\n",
    "df_kids.drop(['kid_count'], axis=0, inplace=True)\n",
    "\n",
    "# Export to Excel for 100% stacked graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. When are members' children born?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare birth patterns by month for first and second child\n",
    "\n",
    "# x0 = dfn.k1bday_month\n",
    "# x1 = dfn.k2bday_month\n",
    "\n",
    "# trace1 = go.Histogram(\n",
    "#     x=x0,\n",
    "#     opacity=0.75\n",
    "# )\n",
    "# trace2 = go.Histogram(\n",
    "#     x=x1,\n",
    "#     opacity=0.75\n",
    "# )\n",
    "# data = [trace1, trace2]\n",
    "# layout = go.Layout(\n",
    "#     barmode='overlay'\n",
    "# )\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "# py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. How did members find out about the Parkside Parenting Partnership?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of how discovered\n",
    "# data = [go.Histogram(x=dfn.discovered)]\n",
    "# py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph showing when Google was the way the group was found by annual membership\n",
    "\n",
    "# google_cols = ['mem_no','joined','discovered_Found it through a Google search']\n",
    "\n",
    "google_cols = ['mem_no','joined','join_year','join_month','discovered_Found it through a Google search']\n",
    "\n",
    "df_ggl = pd.DataFrame(dfn[google_cols])\n",
    "\n",
    "# break into annual columns\n",
    "df10 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2010])\n",
    "df10.rename(columns={'join_month':'joined_2010'}, inplace=True)\n",
    "df10g = df10.groupby(['joined_2010'])['join_year'].count()\n",
    "\n",
    "df11 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2011])\n",
    "df11.rename(columns={'join_month':'joined_2011'}, inplace=True)\n",
    "df11g = df11.groupby(['joined_2011'])['join_year'].count()\n",
    "\n",
    "df12 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2012])\n",
    "df12.rename(columns={'join_month':'joined_2012'}, inplace=True)\n",
    "df12g = df12.groupby(['joined_2012'])['join_year'].count()\n",
    "\n",
    "df13 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2013])\n",
    "df13.rename(columns={'join_month':'joined_2013'}, inplace=True)\n",
    "df13g = df13.groupby(['joined_2013'])['join_year'].count()\n",
    "\n",
    "df14 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2014])\n",
    "df14.rename(columns={'join_month':'joined_2014'}, inplace=True)\n",
    "df14g = df14.groupby(['joined_2014'])['join_year'].count()\n",
    "\n",
    "df15 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2015])\n",
    "df15.rename(columns={'join_month':'joined_2015'}, inplace=True)\n",
    "df15g = df15.groupby(['joined_2015'])['join_year'].count()\n",
    "\n",
    "df16 = pd.DataFrame(df_ggl.loc[df_ggl['join_year'] == 2016])\n",
    "df16.rename(columns={'join_month':'joined_2016'}, inplace=True)\n",
    "df16g = df16.groupby(['joined_2016'])['join_year'].count()\n",
    "\n",
    "\n",
    "# combine annual counts into single df\n",
    "df_ggl = pd.concat([df10g, df11g, df12g, df13g, df14g, df15g, df16g], axis=1)\n",
    "df_ggl.columns = ['joined_2010', 'joined_2011', 'joined_2012', 'joined_2013',\\\n",
    "                  'joined_2014', 'joined_2015', 'joined_2016']\n",
    "\n",
    "# change column names\n",
    "df_ggl['month']=('Jan','Feb','Mar','Apr','May','June','July','Aug','Sept','Oct','Nov','Dec')\n",
    "df_ggl\n",
    "\n",
    "# Export to Excel for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bring in Google Trends data\n",
    "goog = pd.read_csv(\"../../projects/psp/raw_data/multiTimeline.csv\")\n",
    "\n",
    "# create graph in plotly\n",
    "\n",
    "trace = go.Scatter(\n",
    "    x = goog['Week'],\n",
    "    y = goog['Park Slope Parents: (New York)']\n",
    ")\n",
    "\n",
    "data = [trace]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5. How popular is each type of membership? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of membership type\n",
    "# data = [go.Histogram(x=dfn.mem_type)]\n",
    "# py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph for membership type broken out by annual membership\n",
    "\n",
    "mem_type_cols = ['mem_no','mem_type', 'mem_2010','mem_2011','mem_2012','mem_2013',\\\n",
    "                  'mem_2014','mem_2015','mem_2016']\n",
    "\n",
    "dfn_mem_type = pd.DataFrame(dfn[mem_type_cols])\n",
    "\n",
    "# count of members with 1 year membership\n",
    "df_mem1 = pd.DataFrame(dfn_mem_type.loc[dfn_mem_type['mem_type'] == 0])\n",
    "mem1_count = df_mem1.sum(axis=0)\n",
    "smem1 = pd.Series(mem1_count)\n",
    "print mem1_count\n",
    "\n",
    "# # count of members with 2 year membership\n",
    "df_mem2 = pd.DataFrame(dfn_mem_type.loc[dfn_mem_type['mem_type'] == 1])\n",
    "mem2_count = df_mem2.sum(axis=0)\n",
    "smem2 = pd.Series(mem2_count)\n",
    "# print due_count\n",
    "\n",
    "# # count of members with 3 year membership\n",
    "df_mem3 = pd.DataFrame(dfn_mem_type.loc[dfn_mem_type['mem_type'] == 2])\n",
    "mem3_count = df_mem3.sum(axis=0)\n",
    "smem3 = pd.Series(mem3_count)\n",
    "# print due_count\n",
    "\n",
    "# # count of members with 5 year membership\n",
    "df_mem5 = pd.DataFrame(dfn_mem_type.loc[dfn_mem_type['mem_type'] == 3])\n",
    "mem5_count = df_mem5.sum(axis=0)\n",
    "smem5 = pd.Series(mem5_count)\n",
    "# print due_count\n",
    "\n",
    "# # count of members with lifetime membership\n",
    "df_memlife = pd.DataFrame(dfn_mem_type.loc[dfn_mem_type['mem_type'] == 5])\n",
    "memlife_count = df_memlife.sum(axis=0)\n",
    "smemlife = pd.Series(memlife_count)\n",
    "# print due_count\n",
    "\n",
    "# concat annual dfs into single df\n",
    "df_memt = pd.concat([smem1, smem2, smem3, smem5, smemlife], axis=1)\n",
    "df_memt.drop(['mem_no','mem_type'], axis=0, inplace=True)\n",
    "\n",
    "# change column names\n",
    "df_memt.columns=('1yr_mem', '2yr_mem', '3yr_mem','5yr_mem','lifetime')\n",
    "\n",
    "# Export to Excel for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6. How long do people keep their memberships?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the average membership duration\n",
    "dfn['mem_duration'] = dfn['exp_year'] - dfn['join_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of membership length\n",
    "# data = [go.Histogram(x=dfn.mem_duration)]\n",
    "# py.iplot(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 7. When do members join the PPP, relative to the arrival of their children?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a column that shows the difference between when people join and when their kids\n",
    "# are born\n",
    "dfn['join_v_birth'] = (dfn['joined']-dfn['kid1_bday']).astype('timedelta64[m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculating average difference between date joined and when child was born\n",
    "# once calculation is done, divide by 1440 to get number of days, then by 30 for months\n",
    "k = ((sum(dfn.join_v_birth) / len(dfn.join_v_birth)))\n",
    "print k/1440\n",
    "print ((k/1440)/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exclude outliers\n",
    "\n",
    "dfk = pd.DataFrame(dfn.loc[dfn['join_v_birth'] < 6220800])\n",
    "dfk = pd.DataFrame(dfk.loc[dfk['join_v_birth'] > -1036800])\n",
    "dfk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of difference between PSP join and 1st child's birth\n",
    "# data = [go.Histogram(x=(dfk.join_v_birth/43200))]\n",
    "# py.iplot(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separating out positives and negatives in join v birth (those who joined pre and post birth)\n",
    "jvb_p = []\n",
    "jvb_n = []\n",
    "jvb_z = []\n",
    "\n",
    "for n in dfk.join_v_birth:\n",
    "    if n > 0:\n",
    "        jvb_p.append(n)\n",
    "    elif n < 0:\n",
    "        jvb_n.append(n)\n",
    "    else:\n",
    "        jvb_z.append(n)\n",
    "        \n",
    "# convert pre, post, and zero to Series\n",
    "\n",
    "jvb_pos = pd.Series(jvb_p)\n",
    "jvb_neg = pd.Series(jvb_n)\n",
    "jvb_zed = pd.Series(jvb_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get descriptive data for members who join post-birth\n",
    "jvb_pos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get descriptive data for members who join pre-birth\n",
    "jvb_pos.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph showing difference between when a member joined and the age of child by year\n",
    "\n",
    "join_v_birth_cols = ['mem_no','join_year','join_v_birth']\n",
    "\n",
    "dfn_join_v_birth = pd.DataFrame(dfn[join_v_birth_cols])\n",
    "\n",
    "dfn_join_v_birth.describe()\n",
    "\n",
    "# calculate months from datetime (mult by 43200)\n",
    "# 1 year = 518400\n",
    "# 2 years = 1036800\n",
    "# 3 years = 1555200\n",
    "# 5 years = 2592000\n",
    "\n",
    "# joined while child due\n",
    "df_due = pd.DataFrame(dfn_join_v_birth.loc[dfn_join_v_birth['join_v_birth'] <= 0])\n",
    "df_due['jvb_range'] = 'joined while due'\n",
    "\n",
    "\n",
    "# joined while child was under 1 year old\n",
    "df_one = pd.DataFrame(dfn_join_v_birth.loc[dfn_join_v_birth['join_v_birth'] > 0])\n",
    "df_one = pd.DataFrame(df_one.loc[df_one['join_v_birth'] <= 51840.0 ])\n",
    "df_one['jvb_range'] = 'joined while child under 1'\n",
    "\n",
    "\n",
    "# joined while child was 1 - 2 years old\n",
    "df_two = pd.DataFrame(dfn_join_v_birth.loc[dfn_join_v_birth['join_v_birth'] > 518400])\n",
    "df_two = pd.DataFrame(df_two.loc[df_two['join_v_birth'] <= 1036800.0 ])\n",
    "df_two['jvb_range'] = 'joined while child 1-2'\n",
    "\n",
    "\n",
    "# joined while child was 2 - 3 years old\n",
    "df_three = pd.DataFrame(dfn_join_v_birth.loc[dfn_join_v_birth['join_v_birth'] > 1036800.0])\n",
    "df_three = pd.DataFrame(df_three.loc[df_three['join_v_birth'] <= 1555200.0 ])\n",
    "df_three['jvb_range'] = 'joined while child 2-3'\n",
    "\n",
    "\n",
    "# joined while child was 3 - 5 years old\n",
    "df_five = pd.DataFrame(dfn_join_v_birth.loc[dfn_join_v_birth['join_v_birth'] > 1555200.0])\n",
    "df_five = pd.DataFrame(df_five.loc[df_five['join_v_birth'] <= 2592000.0 ])\n",
    "df_five['jvb_range'] = 'joined while child 3-5'\n",
    "\n",
    "\n",
    "# joined while child was 5+ years old\n",
    "df_older = pd.DataFrame(dfn_join_v_birth.loc[dfn_join_v_birth['join_v_birth'] > 2592000])\n",
    "df_older['jvb_range'] = 'joined while child 5 or older'\n",
    "\n",
    "\n",
    "# combine annual dfs\n",
    "df_jvb = pd.concat([df_due, df_one, df_two, df_five, df_older], axis=0)\n",
    "# df_kids.drop(['kid_count'], axis=0, inplace=True)\n",
    "df_jvb.head(20)\n",
    "\n",
    "# create pivot table \n",
    "dfn_jvbpiv = pd.pivot_table(df_jvb, values='mem_no', index='join_year', columns='jvb_range',\\\n",
    "               aggfunc=len, fill_value=0)\n",
    "\n",
    "# export to Excel for graphing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 8. When do memberships lapse, relative to the birth of members' last child?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a new column which shows the difference between the member's\n",
    "# expiration date and the 2nd child's birthday (defaults to 1st child if no second)\n",
    "\n",
    "dfn['exp_v_birth'] = (dfn['exp_date']-dfn['kid2_bday']).astype('timedelta64[m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# largest membership length before \"lifetime\" is 5 years so I will exclude any instances of a membership longer than\n",
    "# 5 years. Also excluding records with negative numbers here.\n",
    "\n",
    "dfk2 = pd.DataFrame(dfn.loc[dfn['exp_v_birth'] <= 2628000])\n",
    "dfk2 = pd.DataFrame(dfk2.loc[dfk2['exp_v_birth'] >= 0])\n",
    "\n",
    "# breaking out members who have lapsed from members who are still active\n",
    "\n",
    "df_exp = pd.DataFrame(dfk2.loc[dfk2['status'] == 'Expired'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# histogram of difference between PSP expiration date and 2nd child's arrival for expired members\n",
    "# data = [go.Histogram(x=(df_exp.exp_v_birth/43200))]\n",
    "# py.iplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create graph showing difference between when a memberbership lapsed and the age of youngest child (of 2) by year\n",
    "\n",
    "exp_v_birth_cols = ['mem_no','exp_year','exp_v_birth']\n",
    "\n",
    "dfn_exp_v_birth = pd.DataFrame(dfn[exp_v_birth_cols])\n",
    "\n",
    "dfn_exp_v_birth.describe()\n",
    "\n",
    "# calculate months from datetime (mult by 43200)\n",
    "# 1 year = 518400\n",
    "# 2 years = 1036800\n",
    "# 3 years = 1555200\n",
    "# 5 years = 2592000\n",
    "\n",
    "\n",
    "# exped while child was under 1 year old\n",
    "df_one_exp = pd.DataFrame(dfn_exp_v_birth.loc[dfn_exp_v_birth['exp_v_birth'] <= 51840.0])\n",
    "df_one_exp['exp_range'] = 'exped while child under 1'\n",
    "\n",
    "\n",
    "# exped while child was 1 - 2 years old\n",
    "df_two_exp = pd.DataFrame(dfn_exp_v_birth.loc[dfn_exp_v_birth['exp_v_birth'] > 518400])\n",
    "df_two_exp = pd.DataFrame(df_two_exp.loc[df_two_exp['exp_v_birth'] <= 1036800.0 ])\n",
    "df_two_exp['exp_range'] = 'exped while child 1-2'\n",
    "\n",
    "\n",
    "# exped while child was 2 - 3 years old\n",
    "df_three_exp = pd.DataFrame(dfn_exp_v_birth.loc[dfn_exp_v_birth['exp_v_birth'] > 1036800.0])\n",
    "df_three_exp = pd.DataFrame(df_three_exp.loc[df_three_exp['exp_v_birth'] <= 1555200.0 ])\n",
    "df_three_exp['exp_range'] = 'exped while child 2-3'\n",
    "\n",
    "\n",
    "# exped while child was 3 - 5 years old\n",
    "df_five_exp = pd.DataFrame(dfn_exp_v_birth.loc[dfn_exp_v_birth['exp_v_birth'] > 1555200.0])\n",
    "df_five_exp = pd.DataFrame(df_five_exp.loc[df_five_exp['exp_v_birth'] <= 2592000.0 ])\n",
    "df_five_exp['exp_range'] = 'exped while child 3-5'\n",
    "\n",
    "\n",
    "# exped while child was 5+ years old\n",
    "df_older_exp = pd.DataFrame(dfn_exp_v_birth.loc[dfn_exp_v_birth['exp_v_birth'] > 2592000])\n",
    "df_older_exp['exp_range'] = 'exped while child 5 or older'\n",
    "\n",
    "\n",
    "# combine annual dfs\n",
    "df_evb = pd.concat([df_one_exp, df_two_exp, df_five_exp, df_older_exp], axis=0)\n",
    "\n",
    "# create pivot table\n",
    "dfn_evbpiv = pd.pivot_table(df_evb, values='mem_no', index='exp_year', columns='exp_range',\\\n",
    "               aggfunc=len, fill_value=0)\n",
    "\n",
    "# export to excel for graphing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase II: Descriptions and Predictions of Long versus Short Term Members\n",
    "One of the client's concerns was that membership was flattening (or declining, according to the data she provided me). I thought it would be useful for her to know what features have distinguished long term versus short term membership.\n",
    "\n",
    "I defined a long term member as someone who stayed with the organization for at least 2 years. Median membership is 2 years, which is also close to the threshold I saw was when many folks check out relative to their child's birth (22 months). \n",
    "\n",
    "#### Clasification Modelling\n",
    "I first ran a logistic regression and then a number of ensemble models to determine the best tool to use in answering this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initial set up of dataframes nearly mirrors that of EDA so will not be annotated.\n",
    "\n",
    "dfn = pd.read_csv(\"../../projects/psp/raw_data/PSP_data_4capstone.csv\")\n",
    "dfn.drop([\"address\",'join_reason','city','state'], axis=1, inplace=True)\n",
    "dfn['join_year'] = dfn['joined'].dt.year\n",
    "dfn['join_month'] = dfn['joined'].dt.month\n",
    "dfn['exp_year'] = dfn['exp_date'].dt.year\n",
    "dfn['k1bday_year'] = dfn['kid1_bday'].dt.year\n",
    "dfn['k1bday_month'] = dfn['kid1_bday'].dt.month\n",
    "dfn['k2bday_year'] = dfn['kid2_bday'].dt.year\n",
    "dfn['k2bday_month'] = dfn['kid2_bday'].dt.month\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k1bday_year'] >= 1995])\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k2bday_year'] >= 1995])\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k1bday_year'] < 2018])\n",
    "dfn = pd.DataFrame(dfn.loc[dfn['k2bday_year'] < 2018])\n",
    "dfn['mem_duration'] = ((dfn['exp_date'] - dfn['joined'])/604800000000000).astype(int)\n",
    "jvbcols = ['joined','kid1_bday']\n",
    "jvb_df = pd.DataFrame(dfn[jvbcols])\n",
    "jvb_df['jvb'] = (jvb_df['joined']-jvb_df['kid1_bday'])\n",
    "jvb_df.drop(['joined','kid1_bday'], axis=1, inplace=True)\n",
    "jvb_df['jvb2'] = jvb_df['jvb']/timedelta(1)\n",
    "jvb_df.pop('jvb');\n",
    "def positive(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "jvb_df['jvb_pre'] = jvb_df['jvb2'].apply(positive);\n",
    "jvb_pre_post = pd.Series(jvb_df['jvb_pre']);\n",
    "dfn = pd.concat([dfn, jvb_pre_post], axis=1)\n",
    "dfn.rename (columns={0:'pre-post-birth'}, inplace=True)\n",
    "dfn.info()\n",
    "dfn['exp_v_birth'] = (dfn['exp_date']-dfn['kid2_bday']).astype('timedelta64[m]')\n",
    "dfn.drop(['mem_type','status'], axis=1, inplace=True)\n",
    "dup_dummy = pd.get_dummies(dfn['dup'], prefix='dup')\n",
    "parent_dummy = pd.get_dummies(dfn['parent_status'], prefix='parent_status')\n",
    "gender_dummy = pd.get_dummies(dfn['gender'], prefix='gender')\n",
    "email_dummy = pd.get_dummies(dfn['club_email'], prefix='club_email')\n",
    "advice_dummy = pd.get_dummies(dfn['advice_grp'], prefix='advice_grp')\n",
    "classifieds_dummy = pd.get_dummies(dfn['classifieds'], prefix='classifieds')\n",
    "class_sp_dummy = pd.get_dummies(dfn['classifieds_spouse'], prefix='classifieds_spouse')\n",
    "tony_dummy = pd.get_dummies(dfn['tony_kids'], prefix='tony_kids')\n",
    "disc_dummy = pd.get_dummies(dfn['discovered'], prefix='discovered')\n",
    "kidcount_dummy = pd.get_dummies(dfn['kid_count'], prefix='kid_count')\n",
    "dfn = dfn.join(dup_dummy)\n",
    "dfn = dfn.join(parent_dummy)\n",
    "dfn = dfn.join(gender_dummy)\n",
    "dfn = dfn.join(email_dummy)\n",
    "dfn = dfn.join(advice_dummy)\n",
    "dfn = dfn.join(classifieds_dummy)\n",
    "dfn = dfn.join(class_sp_dummy)\n",
    "dfn = dfn.join(tony_dummy)\n",
    "dfn = dfn.join(disc_dummy)\n",
    "dfn = dfn.join(kidcount_dummy)\n",
    "dfn.rename (columns={\"discovered_A PSP member I don't know told me about it\":\"discovered_PSP mem I do not know\",\\\n",
    "                    \"discovered_I don't remember\":\"discovered_do not remember\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create column for whether a member had belonged for over/under 2 years.\n",
    "\n",
    "# md of 1 == n is a long term member\n",
    "\n",
    "md = []\n",
    "\n",
    "for n in dfn.mem_duration:\n",
    "    if n >= 104:\n",
    "        md.append(1)\n",
    "    else:\n",
    "        md.append(0)\n",
    "        \n",
    "md_s = pd.Series(md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concat long-short to dfn\n",
    "\n",
    "dfn=dfn.set_index(md_s.index)\n",
    "dfn['long_short']=md_s\n",
    "\n",
    "# For reasons I have yet to discover I needed to concat this way and not in the usual manner\n",
    "# because it would add the Series twice, one on top of the other, creating a dataframe\n",
    "# that was twice as long but only had data in the first half. This played havoc with my\n",
    "# later efforts and in order to uncover this as the issue took several days to uncover,\n",
    "# picking it apart piece by piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# discarded several columns to reduce what would be significant colinearity\n",
    "\n",
    "# drop datetime columns\n",
    "dfn_r.drop(['joined','exp_date','last_renewal_date','kid1_bday','kid2_bday','k1bday_year','k2bday_year'], axis=1, inplace=True)\n",
    "\n",
    "# drop columns that are specifically date-related (since we're predicting time) and mem_no, since that is meaningless\n",
    "dfn_r.drop(['mem_no','join_year','exp_year','mem_duration','exp_v_birth','dup_NO','dup_YES'], axis=1, inplace=True)\n",
    "\n",
    "# dropping all parent status because you have to be a parent or expecting to be a member/pre-birth captured elsewhere\n",
    "dfn_r.drop(['parent_status_No','parent_status_No, but we are pregnant/adopting','parent_status_Yes',\\\n",
    "            'kid_count_0.0'],axis=1, inplace=True)\n",
    "\n",
    "# dropping columns that have dummies\n",
    "dfn_r.drop(['dup','parent_status','gender','club_email','advice_grp','classifieds','classifieds_spouse',\\\n",
    "            'tony_kids','discovered','kid_count'],axis=1, inplace=True)\n",
    "\n",
    "# dropping negative of paired dummies\n",
    "dfn_r.drop(['club_email_No','gender_Male','tony_kids_No','advice_grp_0','classifieds_0',\\\n",
    "           'classifieds_spouse_0'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shortened column names\n",
    "\n",
    "dfn_r.rename (columns={\"discovered_PSP mem I do not know\":\"disc_PSP_unknown\",\\\n",
    "                       \"discovered_A PSP member who is a friend/neighbor\": \"disc_friend_neigh\",\\\n",
    "                       \"discovered_Found it through Yahoo\":\"disc_Yahoo\",\\\n",
    "                       \"discovered_Found it through a Google search\":\"disc_Google\",\\\n",
    "                      \"discovered_Heard about it on another online parenting group (Urban Baby, etc.)\":\\\n",
    "                      \"disc_other_par_grp\",\"discovered_Heard about it through a magazine, newspaper, blog\":\\\n",
    "                      \"disc_mag_paper_blog\",\"discovered_do not remember\":\"disc_no_recall\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# move target (long_short) to first column\n",
    "\n",
    "l_s = dfn_r['long_short']\n",
    "dfn_r.drop(labels=['long_short'], axis=1,inplace = True)\n",
    "dfn_r.insert(0, 'long_short', l_s)\n",
    "dfn_r.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a Series of target data for later use\n",
    "s = dfn_r['long_short'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize data\n",
    "scaler = StandardScaler().fit(dfn_r)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train columns\n",
    "train_cols = dfn_r.columns[1:]\n",
    "logit = sm.Logit(dfn_r['long_short'], dfn_r[train_cols], maxiter=500)\n",
    "result = logit.fit()\n",
    "\n",
    "train_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to examine coefficients in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run logistic regression with ridge regression. I had originally run it with Lasso but \n",
    "# realized that was the wrong way to go when I combed back up through my code to understand\n",
    "# why I had such a terrible model (scores in the 50s)\n",
    "\n",
    "model = linear_model.LogisticRegression(penalty = 'l2', C = 10.0) \n",
    "\n",
    "X_list = ['zip','join_month','k1bday_month','k2bday_month','jvb_pre','gender_Female','club_email_Yes',\\\n",
    "          'advice_grp_1','classifieds_1','classifieds_spouse_1','tony_kids_Yes',\\\n",
    "          'disc_PSP_unknown','disc_friend_neigh','disc_Yahoo','disc_Google','disc_other_par_grp',\\\n",
    "          'disc_mag_paper_blog','disc_no_recall','discovered_Other','kid_count_0.5',\\\n",
    "          'kid_count_1.0','kid_count_2.0','kid_count_3.0','kid_count_4.0']\n",
    "\n",
    "\n",
    "X = dfn_r[X_list]\n",
    "y = dfn_r['long_short']\n",
    "\n",
    "print X.shape, y.shape\n",
    "\n",
    "model.fit(X, y)\n",
    "datatitle = examine_coefficients(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# look at coefficients\n",
    "datatitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strongest coefficients (absolute) are:\n",
    "\n",
    "- expectant family (-2.85)\n",
    "- joining pre-birth (-2.25)\n",
    "- 3 children (+1.24)\n",
    "- 2 children (+1.22)\n",
    "\n",
    "after that is a sizeable drop off to:\n",
    "- spouse subscribed to classifieds list (+0.66)\n",
    "- primary member is female (+0.61)\n",
    "- subscribes to advice group (+0.61)\n",
    "- subscribes to club email (+0.56)\n",
    "\n",
    "The negative coefficient for expectant families mean that expectant parents have been the least likely to retain membership past 2 years. This aligns with the 22 months shown as the average number of months a child was when a family let its membership lapse.\n",
    "\n",
    "The client also said that \"Our baby groups die off after the first year so we really need to figure out how to make it more enticing to stay.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# examine correlations between features\n",
    "\n",
    "df_corr = abs(X.corr())\n",
    "\n",
    "df_corr.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_corr.dropna(inplace=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(50,30))   \n",
    "sns.set(font_scale=4)\n",
    "ax = sns.heatmap(df_corr, annot=True, annot_kws={\"size\": 21})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant correlation between featuress (no colinearity), which is really surprising considering there's pre-birth membership and joining while preggers. This pairing is only about 52% correlated, though. Curious, but at this point sure enough of the data that I attribute that more to data entry error and a small difference in how the questions are asked. Will tweak model to see if it improves scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create train/test split cv\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33)\n",
    "\n",
    "print 'Length of X_train: ' + str(len(X_train))\n",
    "print 'Length of y_train: ' + str(len(y_train))\n",
    "print\n",
    "print 'Length of X_test: ' + str(len(X_test))\n",
    "print 'Length of y_test: ' + str(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit model in cv\n",
    "\n",
    "model = linear_model.LogisticRegression(penalty = 'l2', C = 10.0) \n",
    "\n",
    "model.fit(X_test, y_test)\n",
    "datatitle = examine_coefficients(model, X)\n",
    "datatitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get performance measurements\n",
    "\n",
    "for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:\n",
    "    scores = cross_val_score(model, X, y, scoring=metric)\n",
    "    print(\"mean {}: {}, all: {}\".format(metric, scores.mean(), scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross val scores\n",
    "print cross_val_score(model, X, y, n_jobs=1, cv=5)\n",
    "print cross_val_score(model, X, y, n_jobs=1, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find optimal C\n",
    "logreg_cv = LogisticRegressionCV(Cs=20, solver='liblinear', cv=3, penalty='l2', scoring='f1')\n",
    "cv_model = logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "print('best C for class:')\n",
    "best_C = {logreg_cv.classes_[i]:x for i, (x, c) in enumerate(zip(logreg_cv.Cs_,\\\n",
    "                                                                 logreg_cv.classes_))}\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get confusion matrix and classification report\n",
    "\n",
    "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['long-term', 'short-term'],\n",
    "                         columns=['predicted_long','predicted_short'])\n",
    "\n",
    "print(confusion)\n",
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see ROC curve\n",
    "\n",
    "logreg = LogisticRegression(random_state=43)\n",
    "logreg.fit(X_train, y_train)\n",
    "Y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# generic curve plotting function\n",
    "def auc_plotting_function(rate1, rate2, rate1_name, rate2_name, curve_name):\n",
    "    AUC = auc(rate1, rate2)\n",
    "    # Plot of a ROC curve for class 1 (fatality)\n",
    "    plt.figure(figsize=[11,9])\n",
    "    plt.plot(rate1, rate2, label=curve_name + ' (area = %0.2f)' % AUC, linewidth=4)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(rate1_name, fontsize=16)\n",
    "    plt.ylabel(rate2_name, fontsize=16)\n",
    "    plt.title(curve_name + ' short-term', fontsize=14)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# plot receiving operator characteristic curve\n",
    "def plot_roc(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_plotting_function(fpr, tpr, 'False Positive Rate', 'True Positive Rate', 'ROC')\n",
    "    \n",
    "Y_score = logreg.decision_function(X_test)\n",
    "plot_roc(y_test, Y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though generally happy, if not overwhelmed, with the scores received for the model I could not get past the thought that I had both the calculated measurement of members joining before their children were born and a reported response of a member joining before their child was born. What threw me further was that there was so little correlation between the two.\n",
    "\n",
    "The first solution to this was to go back and verify that my calculated feature was done correctly.\n",
    "\n",
    "It was not.\n",
    "\n",
    "The numbers above reflect the work after it had been fixed and yet still I was unsatisfied by having both of those numbers in. I decided to try running everything again two times, one without the calculated version of the answer and one without the reported version. \n",
    "\n",
    "Of the two, removing the calculated feature harmed the model while taking out the self-reported feature improved it. The below code is for the latter, which I used moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run logistic regression without kid_count0.5\n",
    "#limited annotation as it mirrors above\n",
    "\n",
    "model = linear_model.LogisticRegression(penalty = 'l2', C = 10.0) \n",
    "\n",
    "X3_list = ['zip','join_month','k1bday_month','k2bday_month','gender_Female','club_email_Yes',\\\n",
    "          'advice_grp_1','classifieds_1','classifieds_spouse_1','tony_kids_Yes',\\\n",
    "          'disc_PSP_unknown','disc_friend_neigh','disc_Yahoo','disc_Google','disc_other_par_grp',\\\n",
    "          'disc_mag_paper_blog','disc_no_recall','discovered_Other','jvb_pre',\\\n",
    "          'kid_count_1.0','kid_count_2.0','kid_count_3.0','kid_count_4.0']\n",
    "\n",
    "X3 = dfn_r[X3_list]\n",
    "y = dfn_r['long_short']\n",
    "\n",
    "print X3.shape, y.shape\n",
    "\n",
    "model.fit(X3, y)\n",
    "datatitle = examine_coefficients(model, X3)\n",
    "datatitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train/test split cv\n",
    "\n",
    "X3_train, X3_test, y_train, y_test = train_test_split(X3, y, test_size=.33)\n",
    "\n",
    "print 'Length of X_train: ' + str(len(X3_train))\n",
    "print 'Length of y_train: ' + str(len(y_train))\n",
    "print\n",
    "print 'Length of X_test: ' + str(len(X3_test))\n",
    "print 'Length of y_test: ' + str(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression(penalty = 'l2', C = 10.0) \n",
    "\n",
    "model.fit(X3_test, y_test)\n",
    "datatitle = examine_coefficients(model, X3)\n",
    "datatitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the class labels for the Test set\n",
    "\n",
    "y_pred = model.predict(X3_test)\n",
    "print y_pred\n",
    "print \" \"\n",
    "for metric in ['accuracy', 'precision', 'recall', 'roc_auc']:\n",
    "    scores = cross_val_score(model, X3, y, scoring=metric)\n",
    "    print(\"mean {}: {}, all: {}\".format(metric, scores.mean(), scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross val scores\n",
    "print cross_val_score(model, X3, y, n_jobs=1, cv=5)\n",
    "print cross_val_score(model, X3, y, n_jobs=1, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg_cv = LogisticRegressionCV(Cs=20, solver='liblinear', cv=3, penalty='l2', scoring='f1')\n",
    "cv_model = logreg_cv.fit(X3_train, y_train)\n",
    "\n",
    "print('best C for class:')\n",
    "best_C = {logreg_cv.classes_[i]:x for i, (x, c) in enumerate(zip(logreg_cv.Cs_,\\\n",
    "                                                                 logreg_cv.classes_))}\n",
    "print(best_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conmat = np.array(confusion_matrix(y_test, y_pred, labels=[0,1]))\n",
    "\n",
    "confusion = pd.DataFrame(conmat, index=['long-term', 'short-term'],\n",
    "                         columns=['predicted_long','predicted_short'])\n",
    "\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=43)\n",
    "logreg.fit(X3_train, y_train)\n",
    "Y_pred = logreg.predict(X3_test)\n",
    "\n",
    "\n",
    "# generic curve plotting function\n",
    "def auc_plotting_function(rate1, rate2, rate1_name, rate2_name, curve_name):\n",
    "    AUC = auc(rate1, rate2)\n",
    "    # Plot of a ROC curve for class 1 (fatality)\n",
    "    plt.figure(figsize=[11,9])\n",
    "    plt.plot(rate1, rate2, label=curve_name + ' (area = %0.2f)' % AUC, linewidth=4)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(rate1_name, fontsize=16)\n",
    "    plt.ylabel(rate2_name, fontsize=16)\n",
    "    plt.title(curve_name + ' short-term', fontsize=14)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# plot receiving operator characteristic curve\n",
    "def plot_roc(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc_plotting_function(fpr, tpr, 'False Positive Rate', 'True Positive Rate', 'ROC')\n",
    "    \n",
    "Y_score = logreg.decision_function(X3_test)\n",
    "plot_roc(y_test, Y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I had a model I wanted to see if I could improve it at all. My first attempt at doing this was with a GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "C_vals = np.logspace(-5,1,50)\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, cv=5)\n",
    "gs.fit(X3, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best parameters from the GSCV\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# confusion matrix using the GSCV parameters\n",
    "logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "cv_model = logreg.fit(X3_train, y_train)\n",
    "\n",
    "cv_pred = cv_model.predict(X3_test)\n",
    "cm = confusion_matrix(y_test, cv_pred, labels=logreg.classes_)\n",
    "cm = pd.DataFrame(cm, columns=logreg.classes_, index=logreg.classes_)\n",
    "\n",
    "conmat2 = np.array(confusion_matrix(y_test, cv_pred, labels=[0,1]))\n",
    "\n",
    "confusion2 = pd.DataFrame(conmat2, index=['long-term', 'short-term'],\n",
    "                         columns=['predicted_long','predicted_short'])\n",
    "\n",
    "print(confusion2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridsearchCV actually hurt the model slightly.\n",
    "\n",
    "The model now predicts long-term membership correctly 65% of the time (rather than 69% originally).\n",
    "\n",
    "I proceded to run the model through a range of ensembles, none of which improved on my numbers from the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cross-val function used:\n",
    "def do_cross_val(model):\n",
    "    scores = cross_val_score(model, X3, y, cv=5, n_jobs= -1)\n",
    "    return scores.mean(), scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier: .64 mean / .02 std\n",
    "Bagging Classifier: .66 / .03\n",
    "Robust Scaler on Decision Tree: .64 / .02\n",
    "Robust Scaler on Bagging: .66 / .03\n",
    "Random Forest Classifier: .68 mean\n",
    "Extra Trees Classifier: .67\n",
    "AdaBoost Classifier: .71\n",
    "Gradient Boost: .71\n",
    "\n",
    "None of these were better than the logistic regression results so I decided not to bother looking at feature importances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis\n",
    "Running PCA in order to get to clusters that will help my client understand the members' needs and hopefully get them to stay longer and draw new members in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declaring my features and separating out the member numbers\n",
    "\n",
    "x = dfn_r.ix[:,1:24].values\n",
    "y = dfn_r.ix[:,0].values\n",
    "\n",
    "\n",
    "# scaling the feature values so that PCA may work\n",
    "xStand = pd.DataFrame(StandardScaler().fit_transform(x),\\\n",
    "                      columns=dfn_r.columns[1:24])\n",
    "covMat = np.cov(xStand.T)\n",
    "\n",
    "eigenValues, eigenVectors = np.linalg.eig(covMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit standardized data for PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X = pca.fit_transform(xStand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a bunch of pca eigenstuffs\n",
    "\n",
    "covMat = np.cov(X.T)\n",
    "\n",
    "eigenValues, eigenVectors = np.linalg.eig(covMat)\n",
    "\n",
    "eigenPairs = [(np.abs(eigenValues[i]), eigenVectors[:,i]) for i in range(len(eigenValues))]\n",
    "eigenPairs.sort()\n",
    "eigenPairs.reverse()\n",
    "for i in eigenPairs:\n",
    "    print(i[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show what variance is explained by each CP\n",
    "\n",
    "totalEigen = sum(eigenValues)\n",
    "varExpl = [(i / totalEigen)*100 for i in sorted(eigenValues, reverse=True)]\n",
    "cumulvarExpl = np.cumsum(varExpl)\n",
    "\n",
    "print(cumulvarExpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see the PCs and make a DF out of them for future use\n",
    "\n",
    "PCAdf = pd.DataFrame(X, columns=['PC1','PC2','PC3'])\n",
    "j = PCAdf.PC1\n",
    "k = PCAdf.PC2\n",
    "l = PCAdf.PC3\n",
    "\n",
    "PCAdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's see how closely related each PC is to the original features\n",
    "\n",
    "PCPieces = abs(pd.DataFrame(pca.components_, columns = dfn_r.columns[1:24], index = ['PC1', 'PC2', 'PC3']))\n",
    "PCPieces.T.sort_values('PC1', ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# more eigenstuffs\n",
    "\n",
    "eigen_pairs = [[eigenValues[i], eigenVectors[:,i]] for i in range(len(eigenValues))]\n",
    "eigen_pairs.sort(reverse=True)\n",
    "\n",
    "weight_2d_projection = np.hstack((eigen_pairs[0][1].reshape(eigenVectors.shape[1],1),\n",
    "                                  eigen_pairs[1][1].reshape(eigenVectors.shape[1],1)))\n",
    "\n",
    "print 'Weight data 2d PCA projection matrix:\\n', weight_2d_projection;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3d plot didn't show anything clearly so reverted back to 2d on 2 PCs\n",
    "\n",
    "# PC1 vs PC2\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "ax = fig.gca()\n",
    "ax = sns.regplot(j,k,\n",
    "                 fit_reg=False, scatter_kws={'s':70}, ax=ax)\n",
    "\n",
    "ax.set_xlabel('principal component 1', fontsize=14)\n",
    "ax.set_ylabel('principal component 2', fontsize=14)\n",
    "\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12) \n",
    "    \n",
    "ax.set_title('PC1 vs PC2\\n', fontsize=15\n",
    "            )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with DB-SCAN\n",
    "\n",
    "db = DBSCAN(eps =10, min_samples=3)\n",
    "db.fit(xStand)\n",
    "\n",
    "core_samples = db.core_sample_indices_\n",
    "labels = db.labels_\n",
    "\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % silhouette_score(X, labels))\n",
    "\n",
    "# I tuned that epsilon all over the place. Couldn't get it past 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(xStand, labels, random_state=43))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualized on 2 clusters\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = 'k'\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=5)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=col,\n",
    "             markeredgecolor='k', markersize=5)\n",
    "\n",
    "plt.rc('xtick', labelsize=18) \n",
    "plt.rc('ytick', labelsize=18)\n",
    "\n",
    "plt.title('Number of clusters: %d' % n_clusters_, fontsize=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I felt that I could get a third cluster out of the data so I looked to KMeans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first tested to see what k would be best\n",
    "\n",
    "k_list = [2, 3, 4,5,6]\n",
    "\n",
    "for k in k_list:\n",
    "    kmean = KMeans(n_clusters=k)\n",
    "    kmean.fit(dfn_r)\n",
    "    random_state = 43\n",
    "    print k, silhouette_score(dfn_r, kmean.labels_), kmean.inertia_\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Means also seems to prefer 2 clusters because of stronger inertia, but I think having 3 clusters would be more useful so I'm going with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosen_k = 3\n",
    "\n",
    "kmean = KMeans(n_clusters=chosen_k, random_state=43)\n",
    "clusters = kmean.fit(PCAdf)\n",
    "    \n",
    "label = pd.Series(kmean.labels_, name = 'label')\n",
    "\n",
    "kmean_df = pd.concat([PCAdf,label], axis = 1)\n",
    "kmean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualizing k-means with 3 cluster\n",
    "\n",
    "centroid = pd.DataFrame(kmean.cluster_centers_, columns=['PC1', 'PC2', 'PC3'])\n",
    "\n",
    "fig = plt.figure(figsize = (12,9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xs = centroid['PC1'] , ys = centroid['PC2'], zs = centroid['PC3'], \\\n",
    "           s = 100, c = 'black', marker = 'x', linewidth = 4, alpha = 1.0)\n",
    "ax.scatter(xs = kmean_df['PC1'] , ys = kmean_df['PC2'], zs=kmean_df['PC3'], alpha = 0.8, \\\n",
    "           s = 50, c = kmean_df['label'], cmap = 'rainbow')\n",
    "\n",
    "ax.set_xlabel('PC1', fontsize=15)\n",
    "ax.set_ylabel('PC2', fontsize=15)\n",
    "ax.set_zlabel('PC3', fontsize=15)\n",
    "\n",
    "plt.rc('xtick', labelsize=20) \n",
    "plt.rc('ytick', labelsize=20) \n",
    "\n",
    "ax.set_title('K-Means with K=3', fontsize = 20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of members in each cluster\n",
    "\n",
    "kmean_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nice division.\n",
    "\n",
    "# now make a dataframe of the members by cluster label\n",
    "kml = pd.DataFrame(kmean.labels_)\n",
    "\n",
    "# and now a new dataframe that includes the member data with the clusterlabel data\n",
    "dfn_r.reset_index(drop=True, inplace=True)\n",
    "kml.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfc = pd.concat([dfn_r,kml], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a little head cleaning\n",
    "dfc.rename(columns={0:'cluster'}, inplace=True)\n",
    "\n",
    "# give the clusters nicer names\n",
    "def new_name(x):\n",
    "    if x == 0:\n",
    "        return \"Cluster A\"\n",
    "    elif x == 1:\n",
    "        return \"Cluster B\"\n",
    "    else:\n",
    "        return \"Cluster C\"\n",
    "    \n",
    "dfc['cluster_name'] = dfc['cluster'].apply(new_name);\n",
    "dfc.pop('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we're ready to see each cluster individually\n",
    "\n",
    "# isolating Cluster A\n",
    "cluster0 = pd.DataFrame(dfc.loc[dfc['cluster_name'] == 'Cluster A'])\n",
    "\n",
    "# isolating Cluster B\n",
    "cluster1 = pd.DataFrame(dfc.loc[dfc['cluster_name'] == 'Cluster B'])\n",
    "\n",
    "# isolating Cluster C\n",
    "cluster2 = pd.DataFrame(dfc.loc[dfc['cluster_name'] == 'Cluster C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now I'll group the clusters together so I can see what they have in common\n",
    "\n",
    "gb = dfc.groupby(['cluster_name'])[['long_short','join_month','k1bday_month','k2bday_month','jvb_pre','gender_Female',\\\n",
    "                                     'club_email_Yes','advice_grp_1','classifieds_1','classifieds_spouse_1',\\\n",
    "                                     'tony_kids_Yes','disc_PSP_unknown','disc_friend_neigh','disc_Yahoo',\\\n",
    "                                     'disc_Google','disc_other_par_grp','disc_mag_paper_blog','disc_no_recall',\\\n",
    "                                     'discovered_Other','kid_count_1.0','kid_count_2.0','kid_count_3.0',\\\n",
    "                                     'kid_count_4.0']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transforming groupby into a dataframe and resetting the index\n",
    "gbdf = pd.DataFrame(gb)\n",
    "gbdf.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows are a series of histograms on features that have meaningful differences between the clusters. Many others were made but discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.long_short)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title('Long Term Member', fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.k1bday_month)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"First Child's Birthday Month\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.jvb_pre)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Joined While Expecting\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.disc_friend_neigh)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Discovered PPP through Member Who is a Friend/Neighbor\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.disc_Google)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Discovered PPP through Google\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.disc_other_par_grp)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"Discovered PPP through Other Parenting Group\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# interesting that the decimal point in the kid counts hasn't been an issue until now\n",
    "\n",
    "gbdf.rename (columns={\"kid_count_1.0\":\"kid_count_1\"}, inplace=True)\n",
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.kid_count_1)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"1 Child\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbdf.rename (columns={\"kid_count_2.0\":\"kid_count_2\"}, inplace=True)\n",
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.kid_count_2)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"2 Children\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbdf.rename (columns={\"kid_count_3.0\":\"kid_count_3\"}, inplace=True)\n",
    "fig = plt.figure(figsize = (4.5,3))\n",
    "\n",
    "objects = (gbdf.cluster_name)\n",
    "y_pos = np.arange(len(gbdf.cluster_name))\n",
    "performance = (gbdf.kid_count_3)\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, fontsize = 14)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.title(\"3 Children\", fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Summaries:\n",
    "\n",
    "\n",
    "### Cluster A:\n",
    "* 36% of members\n",
    "* Cluster A members are more likely to be long-term members than either of the other clusters, which are fairly even split between long and short.\n",
    "* None of these members had 1 child, almost all had 2\n",
    "\n",
    "\n",
    "### Cluster B: \n",
    "* 27% of members\n",
    "* I'm not sure how this could be actionable, but it's interesting that children in this cluster were born later in the year than children in Clusters A and C\n",
    "* These members were far less likely to have joined the group while pregnant than members of the other two clusters.\n",
    "* Members in this cluster were the most likely to have found PPP through a friend or neighbor who is also a member of the group. \n",
    "* The other clusters were much more likely to have found PPP through another parenting group than members of this cluster were.\n",
    "* very few of these members had more than 1 child.\n",
    "\n",
    "\n",
    "### Cluster C:\n",
    "* 37% of members\n",
    "* These members were much more likely to have found PPP through Google than the other clusters were\n",
    "* Almost all of these members were single-child families.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "While the provided data show that membership in Park Slope Parents is on the decline, the organization is taking proactive steps towards expanding its offerings without adding to cost layouts.\n",
    "\n",
    "The average family has 1.37 children and the instances of 1-child families has been on the rise.\n",
    "\n",
    "The group has done well with minimal marketing as a clear majority of its members found out about it through word of mouth among members who are friends or neighbors. The second most popular means of discovery was through Google search, so investment in SEO (search engine optimization) may be a low-cost means of improving membership, especially if there is a member willing to volunteer to take this on.\n",
    "\n",
    "On average families retain their membership for 2.83 years. More and more families have been joining while their children are still due with half of the membership in 2016 falling into this category.\n",
    "\n",
    "Among those who join after their child is born, the median age is 22 months old. 22 months is also the median age at which parents allow their memberships to lapse. This is a _very_ significant observation as this \"ships in the night\" effect means that adding some insentive around that age should both retain members who might otherwise leave as well as make parents who are already likely to join even more likely to.\n",
    "\n",
    "PSP's members can be broken down into 3 clusters:\n",
    "\n",
    "Cluster A are the most loyal, are the most to stay for over 2 years, and all have more than 1 child.\n",
    "\n",
    "Cluster B families almost all have 1 child and are far less likely than the other groups to join before their children are due. This is the smallest cluster (27% of all members versus 36% for Cluster A and 37% for Cluster C)\n",
    "\n",
    "Cluster C families were the most likely to have found PSP through a Google search and are almost all single-family homes.\n",
    "\n",
    "A model has been created that will predict with 79% accuracy whether a family will be a long-term or short-term member. \n",
    "\n",
    "According to this model families who join while they are pregnant are the least likely to maintain their memberships for over 2 years, again emphasizing the need to strengthen services around this age. A surprisingly strong predictor was whether or not the secondary member subscribed to the classifieds service so this is another area where PSP should bolster.\n",
    "\n",
    "As parents needs change over time this model should be updated periodically, especially to monitor whether changes in PSP's services have an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Future Research\n",
    "\n",
    "### Phase III: NLP\n",
    "\n",
    "In the next phase of this project I will collect the text from posts on the organizations website, its Yahoo groups, and its Slack channel to see what topics are of most importance to PSP's members.\n",
    "\n",
    "### Phase IV Demographic Shifts\n",
    "Park Slope and its surrounding neighborhoods have seen dramatic demographic shifts over the time that PSP has been in existance. I will look at what other neighborhoods are experiencing similar shifts to help guide PSP in marketing efforts.\n",
    "\n",
    "### Phase V: Membership Price Analysis\n",
    "I will look into PSP's price tiers and membership offerings and advise on what changes might be made to maximize the group's income and membership."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
